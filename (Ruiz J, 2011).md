## Key Ideas: 
- Gestures in general (as in, gestures have lots of agreement) 
- Gestures in 3D space, the mobile as the input device. 
- Classifying motion gestures in 3D space
- Tilting on the mobile device
- Types of gestures: mimic normal use, real world metaphor, natural consistent mapping
- Participants want the viewport to be visible, they lose feedback otherwise
- They want "discrete" navigation, not based on the amount of "force" used in the gesture. 


## Relevence to my own research: 
- Gestures in general 
- Integration with smartphone, AR integrate with smartphone
- Data collection, motion sensors on smart phone, audio, rating system for how good the gestures are (linkert scale), presented with a senario then it make the gesture (elicitaion), video recording
- Wobbrock has researchers do the exercise themselves and compart Ruiz did not. I like the idea of comparing the 2 sets 
- Gesture mapping vocabulary, these confuse me. How am I to know which one is good lol. Theirs seems pretty comprehensive for the device, I suppose it depends on the input (theirs is pg 7)
- Agreement scores for gestures to find out which are most "agreed? IMPORTANT! 

## Personal comments 
- I think sensors are more sophisticated now, and integrate more with wearables then when this was written
- ie. apple watch/fit bit detecting dancing ect. 
- I would need to look into industry/more recent examples

## Limitations
- Smartphone - which ones were used? '
- From 2011, much more sophisticated inputs available now, see handwashing from apple watch example 
- Smartphone as primary device, for people with "intuitive/deep" understanding on the interface --> contrast with Wobbrock who used those with no surface experience
- U

## Priority:
- medium! 

## Papers found from this paper to read 
- 23 (guessability studies), 27 (communication), 15 (collaberative gestures), 24 (AR gestures and commands), 10 (gulf of execution), 30 (peephole system?)


## Categories: 
- Gestures
- Guessability studies **New concept! 
- Sensors 
